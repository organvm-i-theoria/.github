import unittest
from unittest.mock import MagicMock, patch
from web_crawler import OrganizationCrawler

class TestLinkExtraction(unittest.TestCase):
    def setUp(self):
        self.crawler = OrganizationCrawler()

    def test_extract_markdown_links(self):
        """Test extraction of URLs from markdown link syntax"""
        content = "[GitHub](https://github.com) and [Example](https://example.com)"
        urls = self.crawler._extract_links(content)
        self.assertEqual(set(urls), {"https://github.com", "https://example.com"})

    def test_extract_bare_urls(self):
        """Test extraction of bare URLs"""
        content = "Check out https://github.com and https://example.com for more"
        urls = self.crawler._extract_links(content)
        self.assertEqual(set(urls), {"https://github.com", "https://example.com"})

    def test_mixed_markdown_and_bare_urls(self):
        """Test extraction of both markdown and bare URLs in same content"""
        content = "[Link](https://github.com) and bare https://example.com"
        urls = self.crawler._extract_links(content)
        self.assertEqual(set(urls), {"https://github.com", "https://example.com"})

    def test_trailing_parenthesis_not_included(self):
        """Test that trailing parentheses in bare URLs are not included"""
        content = "See https://example.com/path) for details"
        urls = self.crawler._extract_links(content)
        self.assertEqual(urls, ["https://example.com/path"])

    def test_markdown_url_with_special_chars(self):
        """Test markdown links with special characters in URL"""
        content = "[API Docs](https://api.example.com/v1/users?id=123&active=true)"
        urls = self.crawler._extract_links(content)
        self.assertEqual(urls, ["https://api.example.com/v1/users?id=123&active=true"])

    def test_markdown_url_without_spaces(self):
        """Test that markdown URLs with spaces are excluded"""
        content = "[Bad Link](https://example.com/path with spaces)"
        urls = self.crawler._extract_links(content)
        # Should extract only the part before the space
        self.assertEqual(urls, ["https://example.com/path"])

    def test_nested_parentheses_in_bare_url(self):
        """Test that bare URLs don't capture closing parentheses"""
        content = "(see https://example.com/page)"
        urls = self.crawler._extract_links(content)
        self.assertEqual(urls, ["https://example.com/page"])

    def test_deduplication(self):
        """Test that duplicate URLs are removed"""
        content = "[Link1](https://example.com) and [Link2](https://example.com) and https://example.com"
        urls = self.crawler._extract_links(content)
        self.assertEqual(len(urls), 1)
        self.assertEqual(urls, ["https://example.com"])

    def test_empty_content(self):
        """Test extraction from empty content"""
        urls = self.crawler._extract_links("")
        self.assertEqual(urls, [])

    def test_no_urls(self):
        """Test extraction when no URLs present"""
        content = "This is just plain text with no URLs"
        urls = self.crawler._extract_links(content)
        self.assertEqual(urls, [])

    def test_markdown_with_brackets_in_text(self):
        """Test markdown links with brackets in link text"""
        content = "[Text [with] brackets](https://example.com)"
        urls = self.crawler._extract_links(content)
        self.assertEqual(urls, ["https://example.com"])

    def test_url_with_trailing_punctuation(self):
        """Test that bare URLs may include some trailing punctuation in the match"""
        content = "Visit https://example.com, https://github.com. and https://test.com!"
        urls = self.crawler._extract_links(content)
        # Note: Current regex may capture some trailing punctuation
        # The validation phase will handle these appropriately
        self.assertEqual(len(urls), 3)
        self.assertTrue(any("example.com" in url for url in urls))
        self.assertTrue(any("github.com" in url for url in urls))
        self.assertTrue(any("test.com" in url for url in urls))

class TestWebCrawlerSecurity(unittest.TestCase):
    def setUp(self):
        self.crawler = OrganizationCrawler()
        # Mock urllib3 PoolManager
        self.crawler.http = MagicMock()

    def test_ssrf_protection_private_ip(self):
        """Verify that private IPs are blocked."""
        url = "http://169.254.169.254/latest/meta-data/"
        # We expect _check_link to return 403 and NOT call http.request
        status = self.crawler._check_link(url)
        self.assertEqual(status, 403)
        self.crawler.http.request.assert_not_called()

    def test_ssrf_protection_localhost(self):
        """Verify that localhost is blocked."""
        url = "http://localhost:8080/admin"
        status = self.crawler._check_link(url)
        self.assertEqual(status, 403)
        self.crawler.http.request.assert_not_called()

    @patch('socket.getaddrinfo')
    def test_ssrf_protection_public_ip(self, mock_getaddrinfo):
        """Verify that public IPs are allowed."""
        url = "http://example.com"

        # Mock getaddrinfo to return a public IP (e.g., 93.184.216.34)
        mock_getaddrinfo.return_value = [(2, 1, 6, '', ('93.184.216.34', 80))]

        # Mock http.request to return 200
        mock_response = MagicMock()
        mock_response.status = 200
        self.crawler.http.request.return_value = mock_response

        status = self.crawler._check_link(url)
        self.assertEqual(status, 200)
        self.crawler.http.request.assert_called()

if __name__ == '__main__':
    unittest.main()
